{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36d2ba0-f283-45ac-a922-9814051d2ec2",
   "metadata": {},
   "source": [
    "## Динамические алгоритмы и внешнее обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5374a-faf8-41c7-979a-edab336df68e",
   "metadata": {},
   "source": [
    "Внешнее обучение - подход, который даёт возможность работать с крупными наборами данных, постепенно подгоняя классификатор на меньших пакетах из набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289612b-a30d-43f0-929e-222c239a0b8e",
   "metadata": {},
   "source": [
    "Функция partial_fit (SGDClassifier) используется для потоковой передачи документов прямо из локалього диска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c45eb-8941-4ad8-9730-dcfcd6dbe114",
   "metadata": {},
   "source": [
    "Функция tokenizer очищает необработанные текстовые данные из файла и разбивает его на лексемы, одновременно удаляя стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1963d99-7571-43ea-bc48-5bfa048c195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(<?::|;|=) (?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                  + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51004955-b425-498d-a953-dcca0de790e5",
   "metadata": {},
   "source": [
    "Генераторная функция stream_docs - читает и возвращает один документ за раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e34cba10-4058-48c5-931f-47a50bee77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # пропустить заголовок\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label  # возвращение генератора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df8ea379-1971-40a7-aa13-5eee32d41946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0b705-fe10-43e2-8e6d-42c10d496ba1",
   "metadata": {},
   "source": [
    "Функция get_minibatch принимает поток документов от функции stream_docs и возвращает определенное число документов, указанное в параметре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5b5ef53-d1e3-4c6f-bc02-ed5aca4fb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36564d22-92fb-44cf-b00b-3f9b127244d4",
   "metadata": {},
   "source": [
    "Класс CountVectorizer требует удержания полного глоссария в памяти. Классу TfidVectorizer необходимо хранить в памяти все векторы признаков обучающего набора, чтобы вычислять обратные частоты документов.\n",
    "\n",
    "HashingVectorizer не зависит от данных и задействует трюк с хешированием через 32-битную функцию MurmurHash3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "beee1b38-2c30-4165-a200-25b1a2010b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb5816-6306-4235-b408-05bed5d9147c",
   "metadata": {},
   "source": [
    "Внешнее обучение с выводом прогрессбара:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6305289-54c4-44ae-9be1-93501f94d1cd",
   "metadata": {},
   "source": [
    "Проходим по 45 минипакетам документов, где каждый минипакет состоит из 1000 документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b540b071-d475-40c9-b1ea-d35163693cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:21\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2aa6f-f376-4cbd-9ca6-b41e2aa11655",
   "metadata": {},
   "source": [
    "Применяем последние 5000 документов для оценки эффективности модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b0654bd-2d41-4364-8c40-20de901ef025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.8682\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('acc: ', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05dcf56e-5be0-4a56-b784-c4ab5ccbb45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.8838\n"
     ]
    }
   ],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e37305-7516-4b9d-a893-ba8d2b715921",
   "metadata": {},
   "source": [
    "Более современной альтернативой модели суммирования слов является word2vec.\n",
    "\n",
    "Алгоритм word2vec - это алгоритм обучения без учителя, основанный на нейронных сетях, который пытается автоматически узнать\n",
    "взаимосвязь между словами. Идея, лежащая в основе word2vec, заключается в том, чтобы помещать слова с похожим смыслом в подобные кластеры, и посредством искусной организации пространства векторов модель способна воспроизводить определенные слова с использованием простой векторной математики, например, king - man + wоman = queen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc392bc-6f35-49d5-ad1a-63bf10f504c9",
   "metadata": {},
   "source": [
    "## Тематическое моделирование с помощью латентного размещения Дирихле"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df9419-bc1d-4eb7-96c6-fdb8ca012b7d",
   "metadata": {},
   "source": [
    "Тематическое моделирование - задача кластеризации.\n",
    "\n",
    "Latent Dirichlet Allocation (LatDA) - алгоритм без учителя.\n",
    "LatDA - порождающая вероятностная модель, которая пытается отыскать группы слов, часто появляющихся вместе в различных документах.\n",
    "\n",
    "На вход LatDA получает модель суммирования слов и разлагает ее на 2 новые матрицы:\n",
    "- матрица отображения документов на темы\n",
    "- матрица отображения слов на темы\n",
    "\n",
    "LatDA разлагает матрицу суммирования слов на 2 матрицы таким образом, что если мы перемножим эти две матрицы, то будем в состоянии воспроизвести вход, т.е. матрицу суммирования слов, с самой низкой возможной ошибкой. На практике нас интересуют темы, которые прием LatDA нашел в матрице суммирования слов.\n",
    "Количество тем должно быть определено заранием (является гиперпараметром LatDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42814c3a-3a5a-4879-b45d-120129e6bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fd88a31-8869-4cbe-87f2-e4594b3bf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1749da-2788-45c1-85be-cc9d1c0e745b",
   "metadata": {},
   "source": [
    "Максимальная частота слов, подлежащая расмотрению равна 10% (max_df=0.1), чтобы исключить слова, которые встречаются в документах слишком часто.\n",
    "\n",
    "Также ограничивается количество учитываемых слов пятью тысячами чаще всего встречающихся слов (max_features=5000) для установлениия лимита на размерность набора данных, чтобы улучшить выведение, выполняемое LatDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "851cf8cd-2cef-42bb-b223-3bfff8fe9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "latda = LatentDirichletAllocation(n_components=10,\n",
    "                                  random_state=123,\n",
    "                                  learning_method='batch',\n",
    "                                  n_jobs=-1)\n",
    "X_topics = latda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e423cf-6dc9-4511-b835-125904648b2c",
   "metadata": {},
   "source": [
    "Матрица, содержащая значения важности слов для каждой из 10 тем в порядке возрастания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d7d1a17-2a01-4fa7-b51d-78d0217020b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42d524-bb73-4f6f-acbb-65f101b661d1",
   "metadata": {},
   "source": [
    "Вывод пяти самых важных слов для каждой из 10 тем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3096c370-4737-4255-88eb-ddf64e7899fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема: 1\n",
      "worst minutes script awful stupid\n",
      "Тема: 2\n",
      "family mother father children girl\n",
      "Тема: 3\n",
      "american dvd music tv war\n",
      "Тема: 4\n",
      "human audience cinema art feel\n",
      "Тема: 5\n",
      "police guy car dead murder\n",
      "Тема: 6\n",
      "horror house sex woman girl\n",
      "Тема: 7\n",
      "role performance comedy actor performances\n",
      "Тема: 8\n",
      "series episode war episodes season\n",
      "Тема: 9\n",
      "book version original effects special\n",
      "Тема: 10\n",
      "action fight guy guys fun\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_i, topic in enumerate(latda.components_):\n",
    "    print(f'Тема: {topic_i + 1}')\n",
    "    print(' '.join([feature_names[i] for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c0834-595a-4c6c-a9c8-8ffed13a80ec",
   "metadata": {},
   "source": [
    "Предроложение о темах:\n",
    "1) Плохие фильмы (не являются по-настоящему тематичской категорией)\n",
    "2) Семейные фильмы\n",
    "3) Военные фильмы\n",
    "4) Фильмы об искусстве\n",
    "5) Криминальнные фильмы\n",
    "6) Фильмы ужасов\n",
    "7) Комедийные фильмы\n",
    "8) Фильмы, связанные с телевизионными шоу\n",
    "9) Фильмы по мотивам книг\n",
    "10) Фильмы боевики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0f0410e-2cfd-401e-b5c3-69f9ea537bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horror movie: 1\n",
      "Once upon a time in a castle...... Two little girls are playing in the garden's castle. They are sisters. A blonde little girl (Kitty) and a brunette one (Evelyn). Evelyn steals Kitty's doll. Kitty pursues Evelyn. Running through long corridors, they reach the room where their grandfather, sitting o ...\n",
      "horror movie: 2\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "horror movie: 3\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for i, movie_i in enumerate(horror[:3]):\n",
    "    print(f'horror movie: {i + 1}')\n",
    "    print(df['review'][movie_i][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0faa9b-3072-4ce0-ac64-394be979d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "341"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
